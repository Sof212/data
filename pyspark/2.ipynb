{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#TD 2: Introduction to Spark"
      ],
      "metadata": {
        "id": "safInymLfI5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why SPARK**\n",
        "\n",
        "Apache Spark is a fast, flexible, and developer-friendly leading platform for large-scale SQL, machine learning, batch processing, and stream processing. It is a data processing framework that can quickly perform processing tasks on huge data sets and on a large number of clusters. It can also distribute data processing tasks across multiple computers, either by itself or in conjunction with other distributed computing tools.\n",
        "\n",
        "Spark is written in Scala, it has APIâ€™s in Python, Java and Scala. We will use the API in Phyton, called Pyspark.\n",
        "\n",
        "There are various types of cluster managers such as Apache Mesos, Hadoop, and Standalone Scheduler.\n",
        "\n",
        "The Standalone Scheduler is a standalone Spark cluster manager enabling the installation of Spark on an empty set of machines.\n"
      ],
      "metadata": {
        "id": "Pv2wgTFEfBBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this TD the basic operation of PySpark will be presented.**\n",
        "\n",
        "**When you'll  find a TO DO: it means you'll need to code. Otherwise simply read and understand the examples**."
      ],
      "metadata": {
        "id": "MvunaNMEU4bU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BUYt66g3opa"
      },
      "source": [
        "# **Set up Apache Spark 3.5.0 on google colab for a quick start.**  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liQpQdl-UeRq"
      },
      "source": [
        "# Installation of Pyspark on Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j #all you need to install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RFY6ueq858Z",
        "outputId": "d06b3011-b785-40c9-e6e5-914ce4523643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1yzyI333blO"
      },
      "source": [
        "#Basics of Pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Basic Dataset"
      ],
      "metadata": {
        "id": "tkOLGWG4XJjd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4FETVSn3iBA",
        "outputId": "c1f992a7-18a4-43b4-816f-b7ce24c4cfd1"
      },
      "source": [
        "# Check the pyspark version\n",
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqyA5WA8OA2o"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#First, you need to create a SparkSession.\n",
        "#SparkSession provides a unified entry point for reading data, executing transformations, and performing various operations on distributed datasets.\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"example_app\").getOrCreate()\n",
        "#appName\" refers to the name given to your Spark application.\n",
        "#master configuration is used to specify the cluster manager for the Spark application.\n",
        "#local[*] Run Spark locally with as many worker threads as logical cores on your machine.\n",
        "#if you do local[N] it will run locally with N threads.\n",
        "\n",
        "#SparkSession will internally creates a SparkContext, that you will need configure using the attribute .sparkContext\n",
        "Context=spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's create a list of 100 elements\n",
        "big_list = range(100)\n",
        "print('big_list',big_list)\n",
        "\n",
        "#The parallelize method is used to create an RDD named rdd from this list.\n",
        "#RDD stands for Resilient Distributed Dataset, and it is a fundamental data structure in Apache Spark.\n",
        "#RDDs are an immutable, fault-tolerant collection of elements that can be processed in parallel across a distributed computing cluster.\n",
        "#Immutable means that once an RDD is created, it cannot be modified or changed directly. You cannot add, remove, or update elements within the same RDD.\n",
        "#Instead, any transformation applied to an RDD (such as map, filter, or flatMap) creates a new RDD, leaving the original RDD unchanged.\n",
        "#Another immutable structure that you should have studied are tuples.\n",
        "#RDDs provide a high-level, abstracted interface for distributed data processing, enabling fault tolerance and efficient parallel processing.\n",
        "rdd = Context.parallelize(big_list, 2)\n",
        "\n",
        "#Spark collect() is used to retrieve all the elements\n",
        "#of the RDD/DataFrame/Dataset (from all nodes) to the driver program.\n",
        "rddCollect = rdd.collect()\n",
        "print('rdd Collect',rddCollect)\n",
        "\n",
        "#You could also you take(N) to print or save a N values\n",
        "# Take the first 5 elements of the RDD and print them\n",
        "rddtake = rdd.take(5)\n",
        "print('rdd Take',rddtake)\n",
        "\n",
        "#We can  use collect() after the application of filter(), map(), groupBy(), all of these are transofmrations.\n",
        "#Transformations in Spark are operations that specify how data should be processed to create a new RDD.\n",
        "#However, these transformations do not immediately perform the computation. Instead, they build a logical plan that Spark executes only when an action,\n",
        "#such as collect or count, is called. This approach ensures efficient resource utilization and optimization before execution.\n",
        "#filter() is a tranformation operation that filters out some values.\n",
        "#In this example we want to save only the odd numbers.\n",
        "#The lambda function takes a single argument x (each element of the RDD) and returns True if x is odd (x % 2 != 0), and False otherwise.\n",
        "odds = rdd.filter(lambda x: x % 2 != 0)\n",
        "print('take 10 element',odds.take(10))\n",
        "\n",
        "oddsCollect = odds.collect()\n",
        "print('odds Collect',oddsCollect)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoUvoi6GPHwi",
        "outputId": "c3e77977-8eee-480c-c33d-bb635c5cac99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "big_list range(0, 100)\n",
            "rdd Collect [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "rdd Take [0, 1, 2, 3, 4]\n",
            "take 10 element [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n",
            "odds Collect [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GroupBy() groups elements by whether they are even or odd\n",
        "grouped_rdd = rdd.groupBy(lambda x: \"Even\" if x % 2 == 0 else \"Odd\")\n",
        "#groupBy transformation groups elements based on a specified key, but it returns an RDD of key-value pairs where the key is the grouping key\n",
        "# and the value is an iterable of the elements that share the same key.\n",
        "#If you only need the grouped keys and want to perform some operation on each group,\n",
        "#you typically use map or other transformations to process the grouped data.\n",
        "#(So, if you try to print now grouped_rdd you'll have an error)\n",
        "\n",
        "#map transformation in PySpark is used to transform each element of an RDD (Resilient Distributed Dataset) using a specified function.\n",
        "#It applies the provided function to each element independently and creates a new RDD with the transformed values.\n",
        "counted_rdd = grouped_rdd.map(lambda x: (x[0], len(x[1])))\n",
        "# Collect and print the result\n",
        "result = counted_rdd.collect()\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT8MbgJ5l2rm",
        "outputId": "1d04b5e8-305d-4f38-ec13-5a6b66d08db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Odd', 50), ('Even', 50)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's close the Context!\n",
        "Context.stop()"
      ],
      "metadata": {
        "id": "egxYOHlVkfOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TO DO 1"
      ],
      "metadata": {
        "id": "W5EuEjoSVxZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "small_list=[1,2,3,4]\n",
        "#TO DO:  Let's create another Context\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"example_app\").getOrCreate()\n",
        "Context =spark.sparkContext\n",
        "#TO DO: Let's parallelize the list\n",
        "nums= Context.parallelize([1,2,3,4])\n",
        "#TO DO: Use map() transformation to apply a square transformation on every element of RDD, so from [1,2,3,4] you will have [1,4,9,16]. The syntax is the same as the one for filter()\n",
        "squared = nums.map(lambda x: x*x)\n",
        "#TO DO: now print the result\n",
        "squaredsmall_list=squared.collect()\n",
        "print(squaredsmall_list)\n",
        "Context.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eDrpfnrQl10",
        "outputId": "3d182b17-2910-4e3c-e9c0-e94ab4f41a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 9, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_list = ['Python', 'programming', 'is', 'awesome!']\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"example_app\").getOrCreate()\n",
        "Context =spark.sparkContext\n",
        "\n",
        "rdd = Context.parallelize(new_list)\n",
        "\n",
        "#TO DO: prints a new list in one line only with words with less than 8 letters using new_list\n",
        "\n",
        "filtered_rdd = rdd.filter(lambda word: len(word) < 8)\n",
        "# Collect and print the result\n",
        "result = filtered_rdd.collect()\n",
        "print(result)\n",
        "# Stop the SparkContext\n",
        "Context.stop()\n"
      ],
      "metadata": {
        "id": "iMDJnpTTc252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d5894f-216f-43cc-97a4-b61c6955dbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8S0vCMlWb4w"
      },
      "source": [
        "\n",
        "## More complex datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"example_app\").getOrCreate()\n",
        "#The dataset now is a bit more complecated, it contains Name, Age and Height of people\n",
        "\n",
        "columns = [\"Name\",\"Age\",\"Height\"]\n",
        "data = [(\"Mark\", 28,183), (\"Jacob\", 23,190), (\"Christine\",31,157),(\"Meridith\",18,167)]\n",
        "\n",
        "# Create DataFrame\n",
        "#A DataFrame is a fundamental data structure in Apache Spark that represents a distributed collection of data organized into named columns.\n",
        "#It is conceptually similar to a table in a relational database or a data frame in R or Python's pandas library.\n",
        "#Spark's DataFrame provides a higher-level, more structured API for distributed data processing compared to Resilient Distributed Datasets (RDDs).\n",
        "dfnocolumns = spark.createDataFrame(data)\n",
        "dfnocolumns.show()\n"
      ],
      "metadata": {
        "id": "o0KnM8Ypjs1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88905840-e6b8-4eb7-c289-0d55245fc5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+---+\n",
            "|       _1| _2| _3|\n",
            "+---------+---+---+\n",
            "|     Mark| 28|183|\n",
            "|    Jacob| 23|190|\n",
            "|Christine| 31|157|\n",
            "| Meridith| 18|167|\n",
            "+---------+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#As you can see in the previouse table, the columns name are not present, to insert this information the following command needs to be used\n",
        "df=dfnocolumns.toDF(*columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZsGP0vRpaRv",
        "outputId": "7d88ab60-9416-48e1-93e4-51412d1d6048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+------+\n",
            "|     Name|Age|Height|\n",
            "+---------+---+------+\n",
            "|     Mark| 28|   183|\n",
            "|    Jacob| 23|   190|\n",
            "|Christine| 31|   157|\n",
            "| Meridith| 18|   167|\n",
            "+---------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to print only the columns you can use\n",
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kfwJ9ZGqsy-",
        "outputId": "262131a5-b073-4b6e-d904-19fbdbdebf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name', 'Age', 'Height']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to print the overall scheme use\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j4LQinIo1ak",
        "outputId": "534828ce-da7d-445b-dff5-ac7f52ffd266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- Height: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to print basic statistics for numeric and string columns in a DataFrame use\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb8qr6jzrAtE",
        "outputId": "f789f2ba-18e8-4614-d6e7-e44bc54b1e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------------+------------------+\n",
            "|summary|     Name|              Age|            Height|\n",
            "+-------+---------+-----------------+------------------+\n",
            "|  count|        4|                4|                 4|\n",
            "|   mean|     NULL|             25.0|            174.25|\n",
            "| stddev|     NULL|5.715476066494082|14.997221964972935|\n",
            "|    min|Christine|               18|               157|\n",
            "|    max| Meridith|               31|               190|\n",
            "+-------+---------+-----------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark internally interprets \"Christine\" as the minimum value (first in lexicographical order) and\n",
        "#\"Meridith\" as the maximum value (last in lexicographical order) for the \"Name\" column."
      ],
      "metadata": {
        "id": "BWiZTAOtaTP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If you just need to count how many samples are in the dataset use\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQDQCJtQLGyY",
        "outputId": "d03f5d24-3817-4f52-d394-8723af63a1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If you want to compute a cross-tabulation of two columns, creating a contingency table use\n",
        "df.crosstab('Age','Height').sort(\"Age_Height\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STol2YSHeLo7",
        "outputId": "1f316c7b-c35f-4760-f792-0464c6e4cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+---+---+---+\n",
            "|Age_Height|157|167|183|190|\n",
            "+----------+---+---+---+---+\n",
            "|        18|  0|  1|  0|  0|\n",
            "|        23|  0|  0|  0|  1|\n",
            "|        28|  0|  0|  1|  0|\n",
            "|        31|  1|  0|  0|  0|\n",
            "+----------+---+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#VectorAssembler is a feature transformer that merges multiple columns into a vector column. outputCol is the new name of the column\n",
        "assembler = VectorAssembler(inputCols=['Age', 'Height'], outputCol = 'Attributes')\n",
        "output = assembler.transform(df)\n",
        "\n",
        "\n",
        "finalized_data = output.select(\"Name\",\"Attributes\")\n",
        "finalized_data.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBC-YuYPVCVx",
        "outputId": "ca67809e-8bfc-4f20-f894-e196b0e8ebb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+\n",
            "|     Name|  Attributes|\n",
            "+---------+------------+\n",
            "|     Mark|[28.0,183.0]|\n",
            "|    Jacob|[23.0,190.0]|\n",
            "|Christine|[31.0,157.0]|\n",
            "| Meridith|[18.0,167.0]|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTGD8jauUlju"
      },
      "source": [
        "\n",
        "## Upload a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nverwd3mUp02"
      },
      "source": [
        "#Either you use wget command and upload it automatically\n",
        "!wget -q https://raw.githubusercontent.com/asifahmed90/pyspark-ML-in-Colab/master/BostonHousing.csv\n",
        "\n",
        "#Or you download BostonHousing.csv from moodle and use this two lines\n",
        "# from google.colab import files\n",
        "# files.upload()\n",
        "\n",
        "#read a file\n",
        "dataset = spark.read.csv('BostonHousing.csv',inferSchema=True, header =True)\n",
        "#dataset = spark.read.json(\".json\") # same with jsons\n",
        "\n",
        "#with text files you'll have to do\n",
        "#sc = pyspark.SparkContext.getOrCreate()\n",
        "#dataset = sc.textFile(\".txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TO DO 2"
      ],
      "metadata": {
        "id": "54UnqwKhJZXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: show the table\n",
        "dataset.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jskHWuAQdtip",
        "outputId": "86bb3c47-be62-48e9-cda7-65038daf9db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
            "|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio|     b|lstat|medv|\n",
            "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
            "|0.00632|18.0| 2.31|   0|0.538|6.575| 65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
            "|0.02731| 0.0| 7.07|   0|0.469|6.421| 78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
            "|0.02729| 0.0| 7.07|   0|0.469|7.185| 61.1|4.9671|  2|242|   17.8|392.83| 4.03|34.7|\n",
            "|0.03237| 0.0| 2.18|   0|0.458|6.998| 45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
            "|0.06905| 0.0| 2.18|   0|0.458|7.147| 54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
            "|0.02985| 0.0| 2.18|   0|0.458| 6.43| 58.7|6.0622|  3|222|   18.7|394.12| 5.21|28.7|\n",
            "|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|\n",
            "|0.14455|12.5| 7.87|   0|0.524|6.172| 96.1|5.9505|  5|311|   15.2| 396.9|19.15|27.1|\n",
            "|0.21124|12.5| 7.87|   0|0.524|5.631|100.0|6.0821|  5|311|   15.2|386.63|29.93|16.5|\n",
            "|0.17004|12.5| 7.87|   0|0.524|6.004| 85.9|6.5921|  5|311|   15.2|386.71| 17.1|18.9|\n",
            "|0.22489|12.5| 7.87|   0|0.524|6.377| 94.3|6.3467|  5|311|   15.2|392.52|20.45|15.0|\n",
            "|0.11747|12.5| 7.87|   0|0.524|6.009| 82.9|6.2267|  5|311|   15.2| 396.9|13.27|18.9|\n",
            "|0.09378|12.5| 7.87|   0|0.524|5.889| 39.0|5.4509|  5|311|   15.2| 390.5|15.71|21.7|\n",
            "|0.62976| 0.0| 8.14|   0|0.538|5.949| 61.8|4.7075|  4|307|   21.0| 396.9| 8.26|20.4|\n",
            "|0.63796| 0.0| 8.14|   0|0.538|6.096| 84.5|4.4619|  4|307|   21.0|380.02|10.26|18.2|\n",
            "|0.62739| 0.0| 8.14|   0|0.538|5.834| 56.5|4.4986|  4|307|   21.0|395.62| 8.47|19.9|\n",
            "|1.05393| 0.0| 8.14|   0|0.538|5.935| 29.3|4.4986|  4|307|   21.0|386.85| 6.58|23.1|\n",
            "| 0.7842| 0.0| 8.14|   0|0.538| 5.99| 81.7|4.2579|  4|307|   21.0|386.75|14.67|17.5|\n",
            "|0.80271| 0.0| 8.14|   0|0.538|5.456| 36.6|3.7965|  4|307|   21.0|288.99|11.69|20.2|\n",
            "| 0.7258| 0.0| 8.14|   0|0.538|5.727| 69.5|3.7965|  4|307|   21.0|390.95|11.28|18.2|\n",
            "+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: show only the columns\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "brR-0HaaK1Yh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfb0d547-a0d5-4058-e2cc-f185dc328ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['crim',\n",
              " 'zn',\n",
              " 'indus',\n",
              " 'chas',\n",
              " 'nox',\n",
              " 'rm',\n",
              " 'age',\n",
              " 'dis',\n",
              " 'rad',\n",
              " 'tax',\n",
              " 'ptratio',\n",
              " 'b',\n",
              " 'lstat',\n",
              " 'medv']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: show the overall scheme\n",
        "dataset.printSchema()"
      ],
      "metadata": {
        "id": "9HORBC_8K1cg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79692962-718f-4566-faec-2591f4b4ed75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- crim: double (nullable = true)\n",
            " |-- zn: double (nullable = true)\n",
            " |-- indus: double (nullable = true)\n",
            " |-- chas: integer (nullable = true)\n",
            " |-- nox: double (nullable = true)\n",
            " |-- rm: double (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- dis: double (nullable = true)\n",
            " |-- rad: integer (nullable = true)\n",
            " |-- tax: integer (nullable = true)\n",
            " |-- ptratio: double (nullable = true)\n",
            " |-- b: double (nullable = true)\n",
            " |-- lstat: double (nullable = true)\n",
            " |-- medv: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: show the basic statistics for numeric and string columns\n",
        "dataset.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzht0etBd4Mx",
        "outputId": "d34cee49-2ab6-41f2-a85d-05c689b8638b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|summary|              crim|                zn|             indus|              chas|                nox|                rm|               age|              dis|              rad|               tax|           ptratio|                 b|             lstat|              medv|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "|  count|               506|               506|               506|               506|                506|               506|               506|              506|              506|               506|               506|               506|               506|               506|\n",
            "|   mean|3.6135235573122535|11.363636363636363|11.136778656126504|0.0691699604743083| 0.5546950592885372| 6.284634387351787| 68.57490118577078|3.795042687747034|9.549407114624506| 408.2371541501976|18.455533596837967|356.67403162055257|12.653063241106723|22.532806324110698|\n",
            "| stddev| 8.601545105332491| 23.32245299451514| 6.860352940897589|0.2539940413404101|0.11587767566755584|0.7026171434153232|28.148861406903595| 2.10571012662761|8.707259384239366|168.53711605495903|2.1649455237144455| 91.29486438415782| 7.141061511348571| 9.197104087379815|\n",
            "|    min|           0.00632|               0.0|              0.46|                 0|              0.385|             3.561|               2.9|           1.1296|                1|               187|              12.6|              0.32|              1.73|               5.0|\n",
            "|    max|           88.9762|             100.0|             27.74|                 1|              0.871|              8.78|             100.0|          12.1265|               24|               711|              22.0|             396.9|             37.97|              50.0|\n",
            "+-------+------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: show how many samples are present\n",
        "print(dataset.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP2FjctWeVAX",
        "outputId": "cd5c63a8-b6fb-4248-fa64-1ed1b7593b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: compute a cross-tabulation of two ('crim','zn' for axample), creating a contingency table use\n",
        "dataset.crosstab('crim','zn').sort(\"crim_zn\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kw2WztBexEx",
        "outputId": "85d62844-de96-42b5-9187-34238e6cb7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|crim_zn|0.0|100.0|12.5|17.5|18.0|20.0|21.0|22.0|25.0|28.0|30.0|33.0|34.0|35.0|40.0|45.0|52.5|55.0|60.0|70.0|75.0|80.0|82.5|85.0|90.0|95.0|\n",
            "+-------+---+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|0.00632|  0|    0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.00906|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|\n",
            "|0.01096|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.01301|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.01311|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|\n",
            "| 0.0136|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|\n",
            "|0.01381|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|\n",
            "|0.01432|  0|    1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.01439|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.01501|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   1|   0|\n",
            "|0.01538|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|\n",
            "|0.01709|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|\n",
            "|0.01778|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|\n",
            "| 0.0187|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
            "|0.01951|  0|    0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|\n",
            "|0.01965|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|\n",
            "|0.02009|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|\n",
            "|0.02055|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|\n",
            "|0.02177|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|\n",
            "|0.02187|  0|    0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   0|   0|   0|   0|   0|   0|   0|\n",
            "+-------+---+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TO DO: merges multiple columns into a vector( use 'crim', 'zn' and to compare \"rm\", \"medv\")\n",
        "\n",
        "\n",
        "#VectorAssembler is a feature transformer that merges multiple columns into a vector column. outputCol is the new name of the column\n",
        "assembler_ex = VectorAssembler(inputCols=['crim', 'zn'], outputCol = 'newAtt')\n",
        "output_ex = assembler_ex.transform(dataset)\n",
        "finalized_data_ex = output_ex.select(\"rm\",\"medv\",\"newAtt\")\n",
        "finalized_data_ex.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqztM6w5eeh-",
        "outputId": "955d8dd2-eee1-4af1-f83e-3b57f4ae8b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+--------------+\n",
            "|   rm|medv|        newAtt|\n",
            "+-----+----+--------------+\n",
            "|6.575|24.0|[0.00632,18.0]|\n",
            "|6.421|21.6| [0.02731,0.0]|\n",
            "|7.185|34.7| [0.02729,0.0]|\n",
            "|6.998|33.4| [0.03237,0.0]|\n",
            "|7.147|36.2| [0.06905,0.0]|\n",
            "| 6.43|28.7| [0.02985,0.0]|\n",
            "|6.012|22.9|[0.08829,12.5]|\n",
            "|6.172|27.1|[0.14455,12.5]|\n",
            "|5.631|16.5|[0.21124,12.5]|\n",
            "|6.004|18.9|[0.17004,12.5]|\n",
            "|6.377|15.0|[0.22489,12.5]|\n",
            "|6.009|18.9|[0.11747,12.5]|\n",
            "|5.889|21.7|[0.09378,12.5]|\n",
            "|5.949|20.4| [0.62976,0.0]|\n",
            "|6.096|18.2| [0.63796,0.0]|\n",
            "|5.834|19.9| [0.62739,0.0]|\n",
            "|5.935|23.1| [1.05393,0.0]|\n",
            "| 5.99|17.5|  [0.7842,0.0]|\n",
            "|5.456|20.2| [0.80271,0.0]|\n",
            "|5.727|18.2|  [0.7258,0.0]|\n",
            "+-----+----+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Manipulation"
      ],
      "metadata": {
        "id": "TZpyvtmwW7x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "address = [(1,\"14851 Jeffrey Rd\",\"DE\"), (2,\"43421 Margarita St\",\"NY\"),(3,\"13111 Siemon Ave\",\"CA\"),(4,\"97643 LetsGo Rd\",\"TX\")]\n",
        "addressSP =spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
        "addressSP.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8bzGttGW64I",
        "outputId": "f8656f5f-67ed-4e91-c543-22acceb74c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+-----+\n",
            "| id|           address|state|\n",
            "+---+------------------+-----+\n",
            "|  1|  14851 Jeffrey Rd|   DE|\n",
            "|  2|43421 Margarita St|   NY|\n",
            "|  3|  13111 Siemon Ave|   CA|\n",
            "|  4|   97643 LetsGo Rd|   TX|\n",
            "+---+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#regexp_replace() you can replace a column value with a string for another string/substring. regexp_replace()\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "addressSP.withColumn('address', regexp_replace('address', 'Rd', 'Road')).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELxwCZIoXR5D",
        "outputId": "02b5e726-bab7-455f-9e5a-940bfc893639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+-----+\n",
            "|id |address           |state|\n",
            "+---+------------------+-----+\n",
            "|1  |14851 Jeffrey Road|DE   |\n",
            "|2  |43421 Margarita St|NY   |\n",
            "|3  |13111 Siemon Ave  |CA   |\n",
            "|4  |97643 LetsGo Road |TX   |\n",
            "+---+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace string column value conditionally\n",
        "#we want to replaced Rd with Road in Jeffrey Rd but not it LetsGo Rd\n",
        "from pyspark.sql.functions import when\n",
        "# when().otherwise() is like if: else:\n",
        "\n",
        "address = [(1,\"14851 Jeffrey Rd\",\"DE\"), (2,\"43421 Margarita St\",\"NY\"),(3,\"13111 Siemon Ave\",\"CA\"),(4,\"97643 LetsGo Rd\",\"TX\")]\n",
        "addressSP = spark.createDataFrame(address, [\"id\", \"address\", \"state\"])\n",
        "# Define the condition for replacement\n",
        "replacement_condition = addressSP.address.contains(\"Jeffrey Rd\")\n",
        "# Use when() and regexp_replace() to conditionally replace \"Rd\" with \"Road\"\n",
        "addressSP = addressSP.withColumn(\"address\", when(replacement_condition, regexp_replace(\"address\", \"Rd\", \"Road\")).otherwise(addressSP.address))\n",
        "# Show the updated DataFrame\n",
        "addressSP.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTBdQCgXX-hi",
        "outputId": "156b69f6-446f-4cb0-ece5-c7a325f772d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+-----+\n",
            "|id |address           |state|\n",
            "+---+------------------+-----+\n",
            "|1  |14851 Jeffrey Road|DE   |\n",
            "|2  |43421 Margarita St|NY   |\n",
            "|3  |13111 Siemon Ave  |CA   |\n",
            "|4  |97643 LetsGo Rd   |TX   |\n",
            "+---+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replace column values from the python dictionary (map)\n",
        "#we replace the string value of the state column with the full abbreviated name from a dictionary key-value pair\n",
        "stateDic={'CA':'California','NY':'New York','DE':'Delaware','TX':'Texas'}\n",
        "addressSP2=addressSP.rdd.map(lambda x:\n",
        "    (x.id,x.address,stateDic[x.state])\n",
        "    ).toDF([\"id\",\"address\",\"state\"])\n",
        "addressSP2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFXMVq4HYXnE",
        "outputId": "92ee0503-a754-4fad-bd89-6a81275207eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+----------+\n",
            "| id|           address|     state|\n",
            "+---+------------------+----------+\n",
            "|  1|14851 Jeffrey Road|  Delaware|\n",
            "|  2|43421 Margarita St|  New York|\n",
            "|  3|  13111 Siemon Ave|California|\n",
            "|  4|   97643 LetsGo Rd|     Texas|\n",
            "+---+------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import translate\n",
        "#translate() string function you can replace character by character of DataFrame column value.\n",
        "addressSP2.withColumn('address', translate('address', '123', 'ABC')) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SDGYcEPY3RB",
        "outputId": "f82c4319-fcab-48cd-e717-390a057c0c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+----------+\n",
            "|id |address           |state     |\n",
            "+---+------------------+----------+\n",
            "|1  |A485A Jeffrey Road|Delaware  |\n",
            "|2  |4C4BA Margarita St|New York  |\n",
            "|3  |ACAAA Siemon Ave  |California|\n",
            "|4  |9764C LetsGo Rd   |Texas     |\n",
            "+---+------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replace column value with a value from another DataFrame column\n",
        "#we match the value from col2 in col1 and them with the ones in col3, creatin new_column\n",
        "#Replace column with another column\n",
        "from pyspark.sql.functions import expr\n",
        "col = spark.createDataFrame(\n",
        "   [(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")],\n",
        "    (\"col1\", \"col2\",\"col3\")\n",
        "  )\n",
        "col.show()\n",
        "col.withColumn(\"new_column\",\n",
        "              expr(\"regexp_replace(col1, col2, col3)\")\n",
        "              ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmijuRrvZCEU",
        "outputId": "94d21dbd-d727-4878-b016-edfd9d7ee3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----+----+\n",
            "|     col1|col2|col3|\n",
            "+---------+----+----+\n",
            "|ABCDE_XYZ| XYZ| FGH|\n",
            "+---------+----+----+\n",
            "\n",
            "+---------+----+----+----------+\n",
            "|     col1|col2|col3|new_column|\n",
            "+---------+----+----+----------+\n",
            "|ABCDE_XYZ| XYZ| FGH| ABCDE_FGH|\n",
            "+---------+----+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's close the Session!\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "VmFbrJ4Vj-Dm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}